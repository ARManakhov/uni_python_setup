{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5838cc82d94a07",
   "metadata": {},
   "source": [
    "1. Скачать видео:\n",
    "https://www.youtube.com/watch?v=NdSqAAT28v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T04:27:38.783664Z",
     "start_time": "2024-06-05T04:26:52.475481Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=NdSqAAT28v0\r\n",
      "[youtube] NdSqAAT28v0: Downloading webpage\r\n",
      "[youtube] NdSqAAT28v0: Downloading ios player API JSON\r\n",
      "[youtube] NdSqAAT28v0: Downloading player 4b63a6a1\r\n",
      "[youtube] NdSqAAT28v0: Downloading m3u8 information\r\n",
      "[info] NdSqAAT28v0: Downloading 1 format(s): 244+251\r\n",
      "[download] Destination: Techno⧸Tech House Mix by DJ Haluk Arslan with Shadow Dancers [NdSqAAT28v0].f244.webm\r\n",
      "\u001B[K[download] 100% of  275.72MiB in \u001B[1;37m00:00:34\u001B[0m at \u001B[0;32m7.90MiB/s\u001B[0m0;33m00:00\u001B[0m0m\r\n",
      "[download] Destination: Techno⧸Tech House Mix by DJ Haluk Arslan with Shadow Dancers [NdSqAAT28v0].f251.webm\r\n",
      "\u001B[K[download] 100% of   74.04MiB in \u001B[1;37m00:00:07\u001B[0m at \u001B[0;32m9.59MiB/s\u001B[0m0;33m00:00\u001B[0m0m\r\n",
      "[Merger] Merging formats into \"Techno⧸Tech House Mix by DJ Haluk Arslan with Shadow Dancers [NdSqAAT28v0].webm\"\r\n",
      "Deleting original file Techno⧸Tech House Mix by DJ Haluk Arslan with Shadow Dancers [NdSqAAT28v0].f251.webm (pass -k to keep)\r\n",
      "Deleting original file Techno⧸Tech House Mix by DJ Haluk Arslan with Shadow Dancers [NdSqAAT28v0].f244.webm (pass -k to keep)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!yt-dlp 'https://www.youtube.com/watch?v=NdSqAAT28v0' -o 'video.webm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c361a8c1ebfc2",
   "metadata": {},
   "source": [
    "2. Средствами библиотек Python извлечь из видео кадры (в оригинале извлекался каждый третий кадр)."
   ]
  },
  {
   "cell_type": "code",
   "id": "2c0f6f5aed1a6954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T14:44:45.564548Z",
     "start_time": "2024-06-05T14:44:45.561188Z"
    }
   },
   "source": [
    "import cv2\n",
    "\n",
    "vidcap = cv2.VideoCapture('video.webm')"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "2a56d9eb4e81b6f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T14:51:51.179069Z",
     "start_time": "2024-06-05T14:44:45.591070Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('video_frames', exist_ok=True)\n",
    "\n",
    "frame_margin = 3\n",
    "\n",
    "success, image = vidcap.read()\n",
    "count = 1\n",
    "alt_count = 1\n",
    "while success:\n",
    "    if count % frame_margin == 0:\n",
    "        cv2.imwrite(os.path.join(\"video_frames\", f\"image_{alt_count}.png\"), image)\n",
    "        alt_count += 1\n",
    "    success, image = vidcap.read()\n",
    "    count += 1\n"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "id": "da1fbb51ea14dfb8",
   "metadata": {},
   "source": [
    "3. Средствами библиотек Python преобразовать разрешение кадров 1920x1080 -> 96x54 (можно другое соотношение)."
   ]
  },
  {
   "cell_type": "code",
   "id": "c23359396f3f433f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:48:27.334872Z",
     "start_time": "2024-06-05T15:43:50.225573Z"
    }
   },
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('video_frames_small', exist_ok=True)\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 5, -1],\n",
    "                   [0, -1, 0]])\n",
    "for image_name in os.listdir('video_frames'):\n",
    "    image = cv2.imread(os.path.join('video_frames', image_name))\n",
    "    image = cv2.filter2D(image, kernel=kernel, ddepth=-5)\n",
    "    image = cv2.resize(image, (96, 54))\n",
    "    cv2.imwrite(os.path.join(\"video_frames_small\", image_name), image)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "2b8f7a63edf3145d",
   "metadata": {},
   "source": [
    "4. Средствами библиотек Python перевести в черно-белое изображение (белый фон, черная фигура)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9d04b94ef2dd600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:54:42.773003Z",
     "start_time": "2024-06-05T15:54:26.872010Z"
    }
   },
   "source": [
    "import cv2, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('video_frames_bw', exist_ok=True)\n",
    "for image_name in os.listdir('video_frames_small'):\n",
    "    image = cv2.imread(os.path.join('video_frames_small', image_name), 1)\n",
    "    tmp = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, alpha = cv2.threshold(tmp, 5, 255, cv2.THRESH_BINARY)\n",
    "    image[alpha.astype(bool), :] = [255, 255, 255]\n",
    "    image[np.logical_not(alpha.astype(bool)), :] = [0, 0, 0]\n",
    "\n",
    "    cv2.imwrite(os.path.join(\"video_frames_bw\", image_name), image)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "f7557954ccecd5e",
   "metadata": {},
   "source": [
    "5. Обучить в Pytorch автоэнкодер (с помощью полносвязных слоев или, как в видео, сверточных) так, чтобы каждое изображение кодировалось вектором длины 128."
   ]
  },
  {
   "cell_type": "code",
   "id": "e3342784f3ce2d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:21:23.815404Z",
     "start_time": "2024-06-05T16:21:23.812973Z"
    }
   },
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "id": "f6c4fceaa6f36e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:21:23.869555Z",
     "start_time": "2024-06-05T16:21:23.844369Z"
    }
   },
   "source": [
    "\n",
    "class DanceImageDataset(Dataset):\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.image_names = os.listdir(base_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(os.path.join(self.base_path, self.image_names[index]), cv2.COLOR_BGR2GRAY)\n",
    "        image = (image[:, :, 1].reshape(96 * 54)\n",
    "                 .astype(np.float32))  # так как по факту у нас и так все чб, можно взять один канал\n",
    "        return image, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "dataset = DanceImageDataset('video_frames_bw')\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=4096,\n",
    "                                     shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "id": "98260af0d7a2da6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:21:23.926047Z",
     "start_time": "2024-06-05T16:21:23.920904Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Creating a PyTorch class\n",
    "# 28*28 ==> 9 ==> 28*28\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(96 * 54, 2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(256, 512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(512, 2048),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(2048, 96 * 54)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "id": "d4ea7afad0a0d64c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:21:24.042981Z",
     "start_time": "2024-06-05T16:21:23.927460Z"
    }
   },
   "source": [
    "model = AE()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-1,\n",
    "                             weight_decay=1e-8)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "id": "eda7081cb3be287c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T16:21:28.540331Z",
     "start_time": "2024-06-05T16:21:24.044247Z"
    }
   },
   "source": [
    "import gc\n",
    "\n",
    "epochs = 50\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch {epoch} started')\n",
    "    for (image, _) in loader:\n",
    "        image = image.to(device)\n",
    "        # Output of Autoencoder\n",
    "        reconstructed = model(image)\n",
    "\n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, image)\n",
    "\n",
    "        # The gradients are set to zero,\n",
    "        # the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Storing the losses in a list for plotting\n",
    "        losses.append(loss.cpu().detach())\n",
    "        # del loss\n",
    "    torch.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "\n",
    "# Defining the Plot Style\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plotting the last 100 values\n",
    "plt.plot(losses)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 started\n",
      "epoch 1 started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[100], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m started\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Output of Autoencoder\u001B[39;49;00m\n",
      "File \u001B[0;32m~/Documents/prog/python_projects/uni_python_setup/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Documents/prog/python_projects/uni_python_setup/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/prog/python_projects/uni_python_setup/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[97], line 7\u001B[0m, in \u001B[0;36mDanceImageDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m----> 7\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCOLOR_BGR2GRAY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     image \u001B[38;5;241m=\u001B[39m (image[:, :, \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m96\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m54\u001B[39m)\n\u001B[1;32m      9\u001B[0m              \u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32))  \u001B[38;5;66;03m# так как по факту у нас и так все чб, можно взять один канал\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m image, image\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "id": "3184bb20a79b8997",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_image = cv2.imread('video_frames_bw/image_12.png')\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(test_image)\n",
    "\n",
    "test_img_reshaped = torch.from_numpy(test_image[:, :, 1]\n",
    "                                     .reshape(96 * 54)\n",
    "                                     .astype(np.float32)).to(device)\n",
    "predicted = model(test_img_reshaped)\n",
    "\n",
    "predicted = predicted.cpu().detach()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(predicted.reshape(54, 96))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = cv2.imread('video_frames_bw/image_12.png', cv2.COLOR_BGR2GRAY)\n",
    "image = np.array(image[:, :, 1]\n",
    "                 .reshape(96 * 54) / 255).astype(np.float32)\n",
    "print(image)"
   ],
   "id": "2bfeb4abc74f7444",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31ce91dd89f12c88",
   "metadata": {},
   "source": [
    "6. Перевести все изображения в последовательность 128-мерных векторов.\n",
    "7. Обучить рекуррентную нейронную сеть (подобрать параметры самостоятельно) предсказывать по текущему следующий вектор.\n",
    "8. С помощью обученной сети сгенерировать последовательность векторов.\n",
    "9. Перевести с помощью декодера последовательность векторов в последовательность кадров.\n",
    "10. Средствами библиотек Python преобразовать кадры в видео."
   ]
  },
  {
   "cell_type": "code",
   "id": "eac4b035ed7c0684",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
